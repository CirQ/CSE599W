{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AAlDtDUt1QP5"
   },
   "source": [
    "# Handwritten Digit Recognition\n",
    "\n",
    "This tutorial guides you through implementing a classic computer vision application: identifying handwritten digits with neural networks. \n",
    "    \n",
    "## Learning points\n",
    "1. How to prepare image data for training?\n",
    "    * batching and color channels\n",
    "    * 2d or 4d tensors for raw image data\n",
    "2. How to use a high-level framework (MXNet/Gluon) to create a neural model?\n",
    "    * fully-connected and convolution layers\n",
    "    * stacking layers\n",
    "3. How to train the model?\n",
    "    * typical training loop\n",
    "    * model evaluation\n",
    "    \n",
    "## Note about neural nets architecture and MXNet API\n",
    "\n",
    "A good source to learn more about why we design neural nets the way we do, and the rationale behind various layers is \n",
    "[Stanford cs231n class](https://cs231n.github.io/).\n",
    "\n",
    "To learn more about MXNet Gluon API, check out\n",
    "[Deep Learning --- The Straight Dope](http://gluon.mxnet.io/index.html)\n",
    "\n",
    "## Prepare data\n",
    "\n",
    "We first download the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, a commonly used dataset for handwritten digit recognition.\n",
    "\n",
    "* Each image in this dataset has been center-cropped to 28x28 with grayscale pixel value [0, 255].\n",
    "* Each image has only a single color channel, rather than three channels as for RGB images.\n",
    "* There are two separate sets of files. The training set contains 60000 examples, and the test set 10000 examples.\n",
    "* Each example includes an image and a label.\n",
    "\n",
    "See more details about the dataset at http://yann.lecun.com/exdb/mnist/.\n",
    "\n",
    "### 2d and 4d tensors for raw images\n",
    "\n",
    "You are familiar with images being stored as a 2d matrix, (width, height). For an image with multiple color channels, you can use a 3d tensor (color_channel, width, height).\n",
    "\n",
    "If we have a bunch of images, we can use 4d tensors to represent them, by adding a new batch_size dimension,\n",
    "\n",
    "**(batch_size, color_channel, width, height)**\n",
    "\n",
    "For example, the grayscale digit image (28, 28) can be stored in memory as (1, 28, 28), and a batch of 100 images can be stored as a **(100, 1, 28, 28)** tensor. \n",
    "    \n",
    "This 4d tensor format for raw images would be later used as input to convolution layer.\n",
    "\n",
    "An alternative way to represent the raw images is to flatten all the dimensions except for the batch_size dimension.\n",
    "\n",
    "For example, (100, 1, 28, 28) can be flattened to (100, 784). This 2d tensor format is later used as input to fully-connected layer.\n",
    "\n",
    "Now, let's first download and load the images and the corresponding labels. Then, we will process the raw images to the appropriate formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 71,
     "output_extras": [
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1272,
     "status": "ok",
     "timestamp": 1522214706090,
     "user": {
      "displayName": "Qiao Zhang",
      "photoUrl": "//lh4.googleusercontent.com/-SXndlxxq5jE/AAAAAAAAAAI/AAAAAAAAErc/EMDhiQCYheU/s50-c-k-no/photo.jpg",
      "userId": "111921404224074339099"
     },
     "user_tz": 420
    },
    "id": "P0cch9SczvXb",
    "outputId": "631582d9-7797-47e4-cb9c-b1da74d33494"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import urllib\n",
    "import gzip\n",
    "import struct\n",
    "\n",
    "def download_data(url, force_download=False):\n",
    "    \"\"\"Download data file to disk and returns filename.\"\"\"\n",
    "    fname = url.split(\"/\")[-1]\n",
    "    if force_download or not os.path.exists(fname):\n",
    "        urllib.request.urlretrieve(url, fname)\n",
    "    return fname\n",
    "\n",
    "def read_data(label_url, image_url):\n",
    "    \"\"\"Download and deserialize raw data to numpy ndarray. Return (label, image) tuple.\"\"\"\n",
    "    # the original files are gzip-compressed with a particular serialization format\n",
    "    with gzip.open(download_data(label_url)) as flbl:\n",
    "        magic, num = struct.unpack(\">II\", flbl.read(8))\n",
    "        label = np.frombuffer(flbl.read(), dtype=np.int8)\n",
    "    with gzip.open(download_data(image_url), 'rb') as fimg:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "        image = np.frombuffer(fimg.read(), dtype=np.uint8).reshape(len(label), rows, cols)\n",
    "    return (label, image)\n",
    "\n",
    "path='http://yann.lecun.com/exdb/mnist/'\n",
    "\n",
    "# label, image tuple\n",
    "(train_lbl, train_img) = read_data(\n",
    "    path + 'train-labels-idx1-ubyte.gz', path + 'train-images-idx3-ubyte.gz')\n",
    "(test_lbl, test_img) = read_data(\n",
    "    path + 't10k-labels-idx1-ubyte.gz', path + 't10k-images-idx3-ubyte.gz')\n",
    "\n",
    "# check raw data\n",
    "\n",
    "# type(train_lbl)\n",
    "# numpy.ndarray\n",
    "# https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.ndarray.html\n",
    "\n",
    "# train_lbl.shape\n",
    "# (60000,)\n",
    "# train_lbl.dtype\n",
    "# dtype('int8')\n",
    "\n",
    "# train_img.shape\n",
    "# (60000, 28, 28)\n",
    "# train_img.dtype\n",
    "# dtype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_tNBPhLJ1NVg"
   },
   "source": [
    "To get a sense for the raw digit images, we can plot the first 10 images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 97,
     "output_extras": [
      {
       "item_id": 1
      },
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 898,
     "status": "ok",
     "timestamp": 1522214709396,
     "user": {
      "displayName": "Qiao Zhang",
      "photoUrl": "//lh4.googleusercontent.com/-SXndlxxq5jE/AAAAAAAAAAI/AAAAAAAAErc/EMDhiQCYheU/s50-c-k-no/photo.jpg",
      "userId": "111921404224074339099"
     },
     "user_tz": 420
    },
    "id": "oLtou7s-z8Qc",
    "outputId": "7c11cc2c-9b3e-4693-96c1-cb95e03faf90"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: [5 0 4 1 9 2 1 3 1 4]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_inst = 10\n",
    "for i in range(num_inst):\n",
    "    plt.subplot(1,num_inst,i+1)\n",
    "    # train_img[i] has shape (28, 28)\n",
    "    plt.imshow(train_img[i], cmap='Greys_r')\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "print('label: %s' % (train_lbl[0:num_inst],))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bb8PIbh21oQI"
   },
   "source": [
    "### Data iterator to iterate raw data by mini-batches\n",
    "\n",
    "We can train neural network by looking at one image/label at a time, but when there is parallelism from the CPU/GPU hardware,it is far more efficient to look at batches of images at a time.\n",
    "\n",
    "To iterate images by batches, we can manually write a for loop and partition the data into chunks and look at one chunk at a time. A simpler way is to wrap the raw data using data iterators typically already provided from deep learning frameworks. \n",
    "\n",
    "Let's use mx.gluon.data.DataLoader and gluon.data.ArrayDataset to create our data iterators.\n",
    "\n",
    "Note that there's an option to shuffle the data rows, which is typically done for training data, but not validation/test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 88,
     "output_extras": [
      {
       "item_id": 3
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 48944,
     "status": "ok",
     "timestamp": 1522214769021,
     "user": {
      "displayName": "Qiao Zhang",
      "photoUrl": "//lh4.googleusercontent.com/-SXndlxxq5jE/AAAAAAAAAAI/AAAAAAAAErc/EMDhiQCYheU/s50-c-k-no/photo.jpg",
      "userId": "111921404224074339099"
     },
     "user_tz": 420
    },
    "id": "Q3VLrgKqzSV_",
    "outputId": "7f73ff1c-96ec-4946-ed89-f69dbf106b85"
   },
   "outputs": [],
   "source": [
    "#!pip install -q mxnet-cu80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KkZqyeniEb2T"
   },
   "source": [
    "In Colab Runtime menu tab, select \"Change runtime type\" and select GPU for Hardware accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 88,
     "output_extras": [
      {
       "item_id": 3
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 47954,
     "status": "ok",
     "timestamp": 1522214215178,
     "user": {
      "displayName": "Qiao Zhang",
      "photoUrl": "//lh4.googleusercontent.com/-SXndlxxq5jE/AAAAAAAAAAI/AAAAAAAAErc/EMDhiQCYheU/s50-c-k-no/photo.jpg",
      "userId": "111921404224074339099"
     },
     "user_tz": 420
    },
    "id": "n67FJo6CCII7",
    "outputId": "ad18c95f-5562-4662-badb-0cdfb9f32a7c"
   },
   "outputs": [],
   "source": [
    "#!pip install -q mxnet-cu80==1.2.0b20180327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "y4046CtUz_oN"
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "mx.random.seed(1)\n",
    "\n",
    "def to4d(img):\n",
    "    \"\"\"Reshape img to 4d tensor and normalize pixel values to [0, 1].\"\"\"\n",
    "    return img.reshape(img.shape[0], 1, 28, 28).astype(np.float32)/255\n",
    "\n",
    "batch_size = 100\n",
    "train_iter = mx.gluon.data.DataLoader(gluon.data.ArrayDataset(to4d(train_img), train_lbl), batch_size, shuffle=True)\n",
    "test_iter = mx.gluon.data.DataLoader(gluon.data.ArrayDataset(to4d(test_img), test_lbl), batch_size)\n",
    "\n",
    "# DataLoader has a method\n",
    "# def __iter__(self):\n",
    "# to see how iterators work, try\n",
    "# for i, (data, label) in enumerate(train_iter):\n",
    "#     print(i, data.shape, label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zoRf6H371vsf"
   },
   "source": [
    "## Multilayer Perceptron (MLP)\n",
    "\n",
    "Now, we are ready to create neural models.\n",
    "\n",
    "One simple model is called multilayer perceptron (MLP).\n",
    "MLP is made up of several fully-connected layers (or called dense layers) stacked one after another, followed by a softmax layer for label prediction.\n",
    "\n",
    "A fully-connected layer is an affine transformation taking input tensor *X* to output tensor *Y*, with two parameters, weight *W* and bias *b*.\n",
    "\n",
    "For the non-batched version, we have the following dimensions for each variable,\n",
    "\n",
    "\\begin{align}\n",
    "\\underset{1xk}{Y} &= \\underset{1xm}{x} \\underset{mxk}{W} + \\underset{1xk}{b}\n",
    "\\end{align}\n",
    "\n",
    "We usually work with batched version, and it has the following dimensions for each variable,\n",
    "\n",
    "\\begin{align}\n",
    "\\underset{nxk}{Y} &= \\underset{nxm}{X} \\underset{mxk}{W} + \\underset{nxk}{B}\n",
    "\\end{align}\n",
    "\n",
    "Note that the bias vector is usually specified as a vector 1xk, rather than a full tensor nxk, and relies on array broadcasting to get the right dimension.\n",
    "\n",
    "The output of a hidden layer is then passed through a non-linear activation layer, e.g., apply sigmoid, tanh or ReLU on output element-wise.\n",
    "\n",
    "The output of the activation layer can be fed as input to the next fully-connected layer. In this way, we can chain multiple full-connected layer.\n",
    "\n",
    "Note that if we do not use activation layer, multiple affine transformation is equivalent to a single affine transformation. Activation layer is necessary to add non-linearity.\n",
    "\n",
    "The last fully-connected layer often has the hidden size equals to the number of classes in the dataset. \n",
    "Then we add a softmax layer, which map the input into a probability score.\n",
    "Assume the input *X* has size *n x m*, the output *Y* would also have size *n x m*, where each row is a normalized probability vector,\n",
    "\n",
    "$$ \\left[\\frac{\\exp(x_{i1})}{\\sum_{j=1}^m \\exp(x_{ij})},\\ldots, \\frac{\\exp(x_{im})}{\\sum_{j=1}^m \\exp(x_{ij})}\\right] $$\n",
    "\n",
    "To evaluate how well the predictions are, we use cross-entropy loss,\n",
    "\n",
    "\\begin{align}\n",
    "L = \\frac{1}{N} \\sum y \\log{\\hat{y}}\n",
    "\\end{align}\n",
    "where *y* is the one-hot vector of true label, and $\\hat{y}$ is the estimated probability from softmax layer.\n",
    "\n",
    "Here's how we would create a multilayer perceptron in MXNet using Gluon frontend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 102,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 330,
     "status": "ok",
     "timestamp": 1522214801219,
     "user": {
      "displayName": "Qiao Zhang",
      "photoUrl": "//lh4.googleusercontent.com/-SXndlxxq5jE/AAAAAAAAAAI/AAAAAAAAErc/EMDhiQCYheU/s50-c-k-no/photo.jpg",
      "userId": "111921404224074339099"
     },
     "user_tz": 420
    },
    "id": "ICKfWyh00BB-",
    "outputId": "6df617fb-226a-45fa-9289-e15209d1b266"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Dense(None -> 128, Activation(relu))\n",
      "  (1): Dense(None -> 64, Activation(relu))\n",
      "  (2): Dense(None -> 10, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "num_examples = 60000\n",
    "\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(128, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(64, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_outputs))\n",
    "\n",
    "# at the end of the 3rd dense layer, there is no activation layer after it\n",
    "# you can use a softmax layer to make predictions or use a softmax_cross_entropy layer to evaluate loss function\n",
    "    \n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "# visualize the network\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uPAEf0YI17te"
   },
   "source": [
    "### Initialize parameters\n",
    "\n",
    "There are many ways to initialize parameters. For different types of layers, we may need different initialization functions.\n",
    "\n",
    "Read more about initialization at http://cs231n.github.io/neural-networks-2/#init."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Lt2p63Yu0Rtf"
   },
   "outputs": [],
   "source": [
    "model_ctx = mx.cpu()\n",
    "# model_ctx = mx.gpu()\n",
    "\n",
    "net.collect_params().initialize(mx.init.Normal(sigma=.1), ctx=model_ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vQ-T9Slu2DnR"
   },
   "source": [
    "### Evaluation metric\n",
    "We can evaluate training/validation set loss by running predictions and compute the average prediction accuracy.\n",
    "\n",
    "Note that we can extract the predicted label from probability vector with argmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "UKjwQQmT0S5v"
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    \"\"\"Make predictions for the dataset and evaluate average accuracy.\"\"\"\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        # reshape to (batch_size, 784)\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        output = net(data)\n",
    "        # skipped a softmax layer since it doesn't change predictions\n",
    "        # use argmax to extract predicted label\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oURJ-ArE2GOX"
   },
   "source": [
    "### Training Loop\n",
    "Now that both the network definition and data iterators are ready. We can start training. \n",
    "\n",
    "We make multiple complete passes over the training data.\n",
    "Each pass is called an epoch.\n",
    "In each epoch, we partition the training data to mini-batches.\n",
    "For each mini-batch, we compute a forward pass of the network to get the predicted output.\n",
    "Then, we do a backward pass, where we compute the loss function and the gradients of each model parameter.\n",
    "We then update each model parameter **w** with its gradient multipled by learning rate,\n",
    "\n",
    "\\begin{align}\n",
    "w = w - \\alpha \\nabla{w}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 187,
     "output_extras": [
      {
       "item_id": 10
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 271729,
     "status": "ok",
     "timestamp": 1522215082481,
     "user": {
      "displayName": "Qiao Zhang",
      "photoUrl": "//lh4.googleusercontent.com/-SXndlxxq5jE/AAAAAAAAAAI/AAAAAAAAErc/EMDhiQCYheU/s50-c-k-no/photo.jpg",
      "userId": "111921404224074339099"
     },
     "user_tz": 420
    },
    "id": "xqTbDHlM0UCP",
    "outputId": "a945e902-0a41-49f6-eba7-4dd09f3a0855"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:53:49] ../src/base.cc:79: cuDNN lib mismatch: linked-against version 8904 != compiled-against version 8101.  Set MXNET_CUDNN_LIB_CHECKING=0 to quiet this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Loss: 1.274676794052124, Train_acc 0.8298166666666666, Test_acc 0.835\n",
      "Epoch 2. Loss: 0.5153365670839946, Train_acc 0.8748333333333334, Test_acc 0.8802\n",
      "Epoch 3. Loss: 0.4045152560551961, Train_acc 0.8933, Test_acc 0.8979\n",
      "Epoch 4. Loss: 0.35663484121958416, Train_acc 0.903, Test_acc 0.9089\n",
      "Epoch 5. Loss: 0.3266846092859904, Train_acc 0.9091, Test_acc 0.9136\n",
      "Epoch 6. Loss: 0.3039308423519135, Train_acc 0.9158666666666667, Test_acc 0.9199\n",
      "Epoch 7. Loss: 0.28589811232884726, Train_acc 0.9204333333333333, Test_acc 0.9253\n",
      "Epoch 8. Loss: 0.27104150257110593, Train_acc 0.9245333333333333, Test_acc 0.9292\n",
      "Epoch 9. Loss: 0.25760475850105286, Train_acc 0.9275, Test_acc 0.9297\n",
      "Epoch 10. Loss: 0.2458699157635371, Train_acc 0.93065, Test_acc 0.9342\n"
     ]
    }
   ],
   "source": [
    "# get a optimizer doing SGD with fixed learning rate\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': .01})\n",
    "\n",
    "epochs = 10\n",
    "smoothing_constant = .01\n",
    "num_examples = 60000\n",
    "\n",
    "for e in range(1, epochs+1):\n",
    "    cumulative_loss = 0\n",
    "    for i, (data, label) in enumerate(train_iter):\n",
    "        # reshape to (batch_size, 784)\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        # this \"with\" record allows us to do automatic differentiation\n",
    "        # basically remembering how the output is compute from the input\n",
    "        # we can use chain rules to find out the gradients of loss function wrt inputs\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        # compute all gradients\n",
    "        loss.backward()\n",
    "        # Perform the weight update.\n",
    "        # Trainer needs to know the batch size of data to normalize the gradients by 1/batch_size.\n",
    "        trainer.step(data.shape[0])\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_iter, net)\n",
    "    train_accuracy = evaluate_accuracy(train_iter, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, cumulative_loss/num_examples, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RHesfE542Kef"
   },
   "source": [
    "### Inference.\n",
    "\n",
    "After training is done, we can make prediction on test images. \n",
    "\n",
    "Let's try it for a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 282,
     "output_extras": [
      {
       "item_id": 1
      },
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 408,
     "status": "ok",
     "timestamp": 1522211790879,
     "user": {
      "displayName": "Qiao Zhang",
      "photoUrl": "//lh4.googleusercontent.com/-SXndlxxq5jE/AAAAAAAAAAI/AAAAAAAAErc/EMDhiQCYheU/s50-c-k-no/photo.jpg",
      "userId": "111921404224074339099"
     },
     "user_tz": 420
    },
    "id": "Z-xUJ6pk0VJP",
    "outputId": "db84550a-0bc2-4343-da38-8c7930b066ba"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted digit is 7\n"
     ]
    }
   ],
   "source": [
    "# show test image\n",
    "plt.imshow(test_img[0], cmap='Greys_r')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# make prediction\n",
    "img = nd.array(to4d(test_img[0:1]).reshape((-1, 784)), ctx=model_ctx)\n",
    "output = net(img)\n",
    "print(\"Predicted digit is\", nd.argmax(output, axis=1).asnumpy().astype(np.int8)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K6JCNZlf2PVq"
   },
   "source": [
    "## Convolutional Neural Networks (ConvNets)\n",
    "\n",
    "ConvNets is a neural architecture that has a reduced model parameter size compared to MLP, and has been shown to achieve higher accuracy on many vision datasets.\n",
    "\n",
    "ConvNets introduces two additional layers, the convolution layer and max-pooling layers, on top of fully-connected layers.\n",
    "\n",
    "Note that the previous fully-connected layer simply flattens the image to a vector, with input dimension (batch_size, 784).\n",
    "It ignores the spatial locality information for pixels near each other.\n",
    "\n",
    "The newly introduced convolutional layer aims to leverage the spatial locality by doing a convolution operation on the input image. \n",
    "The convolution operation can be visualized as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i07pzdO32QsW"
   },
   "source": [
    "<img src=\"https://thatindiandude.github.io/images/conv.png\" style=\"height: 75%; width: 75%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vpu4T5NI2WV9"
   },
   "source": [
    "The convolution is specified by kernel width, kernel height, num_channel, stride and padding.\n",
    "Note that the num_channel has to be the same as the num_channel dimension of the input image.\n",
    "\n",
    "We can also have multiple feature maps, each with their own weight matrices, to capture different features: \n",
    "<img src=\"https://thatindiandude.github.io/images/filters.png\" style=\"height: 75%; width: 75%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2MAyl0Sr2dH5"
   },
   "source": [
    "Besides the convolutional layer, another major innovation of ConvNets is the addition of pooling layers.\n",
    "A pooling layer reduce a $n\\times m$ (often called kernal size) image patch into a single value to make the network less sensitive to the spatial location.\n",
    "\n",
    "<img src=\"https://thatindiandude.github.io/images/pooling.png\" style=\"height: 75%; width: 75%;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 170,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1522214039009,
     "user": {
      "displayName": "Qiao Zhang",
      "photoUrl": "//lh4.googleusercontent.com/-SXndlxxq5jE/AAAAAAAAAAI/AAAAAAAAErc/EMDhiQCYheU/s50-c-k-no/photo.jpg",
      "userId": "111921404224074339099"
     },
     "user_tz": 420
    },
    "id": "GWF1LTcL0rWD",
    "outputId": "45c85003-2e8f-4c31-e516-747ee230ed95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2D(None -> 20, kernel_size=(5, 5), stride=(1, 1), Activation(relu))\n",
      "  (1): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)\n",
      "  (2): Conv2D(None -> 50, kernel_size=(5, 5), stride=(1, 1), Activation(relu))\n",
      "  (3): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)\n",
      "  (4): Flatten\n",
      "  (5): Dense(None -> 512, Activation(relu))\n",
      "  (6): Dense(None -> 10, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_fc = 512\n",
    "num_outputs = 10\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Conv2D(channels=20, kernel_size=5, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
    "    net.add(gluon.nn.Conv2D(channels=50, kernel_size=5, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
    "    # The Flatten layer collapses all axis, except the first one, into one axis.\n",
    "    net.add(gluon.nn.Flatten())\n",
    "    net.add(gluon.nn.Dense(num_fc, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_outputs))\n",
    "\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    \n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wSPOTzF12hHp"
   },
   "source": [
    "Note that LeNet is more complex than the previous multilayer perceptron, so we use GPU instead of CPU for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "BypKXLkz6GoL"
   },
   "outputs": [],
   "source": [
    "model_ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GXnkcxVz5kuX"
   },
   "source": [
    "Don't forget to initialize the model again. This time is a different initialization technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "zZe_P8tJ5q9F"
   },
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=model_ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ftDIurs29i9L"
   },
   "source": [
    "The usual training loop. Note that the raw image data is now fed as a 4d tensor to conv layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 187,
     "output_extras": [
      {
       "item_id": 10
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 692030,
     "status": "ok",
     "timestamp": 1522213960410,
     "user": {
      "displayName": "Qiao Zhang",
      "photoUrl": "//lh4.googleusercontent.com/-SXndlxxq5jE/AAAAAAAAAAI/AAAAAAAAErc/EMDhiQCYheU/s50-c-k-no/photo.jpg",
      "userId": "111921404224074339099"
     },
     "user_tz": 420
    },
    "id": "mDws-lZ_0sop",
    "outputId": "2cb7c03d-d408-42cc-bd57-6c33898c8364"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:54:15] ../src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:96: Running performance tests to find the best convolution algorithm, this can take a while... (set the environment variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Loss: 1.1135810001691182, Train_acc 0.9030333333333334, Test_acc 0.9052\n",
      "Epoch 2. Loss: 0.2738972073952357, Train_acc 0.9349333333333333, Test_acc 0.9401\n",
      "Epoch 3. Loss: 0.18568803434371947, Train_acc 0.9532666666666667, Test_acc 0.958\n",
      "Epoch 4. Loss: 0.14398082388242087, Train_acc 0.9639, Test_acc 0.9662\n",
      "Epoch 5. Loss: 0.11799609728256862, Train_acc 0.9630666666666666, Test_acc 0.9676\n",
      "Epoch 6. Loss: 0.10153435288667678, Train_acc 0.9738, Test_acc 0.9766\n",
      "Epoch 7. Loss: 0.08984460673332215, Train_acc 0.9755666666666667, Test_acc 0.9785\n",
      "Epoch 8. Loss: 0.0810748798718055, Train_acc 0.9771833333333333, Test_acc 0.9775\n",
      "Epoch 9. Loss: 0.07381424021522204, Train_acc 0.9770666666666666, Test_acc 0.9781\n",
      "Epoch 10. Loss: 0.06782159671584766, Train_acc 0.9803, Test_acc 0.9812\n"
     ]
    }
   ],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    \"\"\"Make predictions for the dataset and evaluate average accuracy.\"\"\"\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        # ==== note the difference in raw data input shape ====\n",
    "        # use 4d tensor (batch_size, 1, 28, 28)\n",
    "        data = data.as_in_context(model_ctx)\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': .01})\n",
    "\n",
    "epochs = 10\n",
    "smoothing_constant = .01\n",
    "num_examples = 60000\n",
    "\n",
    "for e in range(1, epochs+1):\n",
    "    cumulative_loss = 0\n",
    "    for i, (data, label) in enumerate(train_iter):\n",
    "        # ==== note the difference in raw data input shape ====\n",
    "        # use 4d tensor (batch_size, 1, 28, 28)\n",
    "        data = data.as_in_context(model_ctx)\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_iter, net)\n",
    "    train_accuracy = evaluate_accuracy(train_iter, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, cumulative_loss/num_examples, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ekTAkGy0FrCb"
   },
   "source": [
    "You should see that ConvNets achieve a higher accuracy than MLP."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "lab1-mnist.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "cse599w",
   "language": "python",
   "name": "cse599w"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
